{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stellar Core Collapse Signal GAN training\n",
    "\n",
    "This demonstrates our training  process for a Generative Adversarial Network (GAN) to generate stellar core collapse signals. The GAN is trained on a dataset of 1764 stellar core collapse signals, each with 256 timestamps. The dataset is augmented using a variety of techniques, including jittering, shifting, scaling, mixing, and window warping. The GAN is trained for 128 epochs, with a batch size of 32. The generator and discriminator are both deep convolutional GANs (DCGANs). The generator has a latent vector size of 100, and the discriminator has a learning rate of 0.00002. The generator has a learning rate of 0.00002. The beta1 hyperparameter for the Adam optimizers is 0.5. The training process takes approximately 2 hours on a single NVIDIA Tesla T4 GPU. The training process is visualised using plots of the generated signals, gradients, and loss. The generator is saved to a file for use in generating new signals.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# critical\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# machine learning\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import entropy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "manualSeed = 99\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# number of channels the signal has\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# number of signals per iteration\n",
    "batch_size = 32\n",
    "\n",
    "# Number of training epochs\n",
    "# num_epochs = int(32 * (6000/1684))\n",
    "# num_epochs = 32\n",
    "num_epochs = 128\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr_g = 0.00002\n",
    "lr_d = 0.00002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# type of gan\n",
    "gans_type = \"dcgans\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWDataset(Dataset):\n",
    "    def __init__(self, signals_csv, parameters_csv):\n",
    "        ### read data from csv files\n",
    "        parameters = pd.read_csv(parameters_csv)\n",
    "        self.signals = pd.read_csv(signals_csv).astype('float32')\n",
    "        # remove unusual parameters\n",
    "        keep_signals_idx = np.array(parameters[parameters['beta1_IC_b'] > 0].index)\n",
    "        parameters = parameters.iloc[:,:]\n",
    "        ###\n",
    "\n",
    "        ### process beta_ic_b parameter\n",
    "        # ranges = [0, 0.06, 0.17, 1]\n",
    "        # labels = [0, 1, 2]\n",
    "        # num_classes = len(labels)\n",
    "        # y = y['beta1_IC_b']\n",
    "        # y = pd.cut(y, bins=ranges, labels=labels).astype('int')\n",
    "        # y = y.values\n",
    "        # y = np.eye(num_classes)[y]\n",
    "        # y = np.reshape(y, (y.shape[0], y.shape[1], 1)).astype('float32')\n",
    "        self.parameters = parameters\n",
    "        ###\n",
    "\n",
    "        # drop corresponding signals which have erroneous parameter values\n",
    "        self.signals = self.signals.iloc[:,keep_signals_idx]\n",
    "        self.signals = self.signals.values\n",
    "        self.augmented_signals = np.empty(shape = (256, 0)).astype('float32')\n",
    "\n",
    "        ### flatten signals and take last 256 timestamps\n",
    "        temp_data = np.empty(shape = (256, 0)).astype('float32')\n",
    "\n",
    "        for i in range(0, self.signals.shape[1]):\n",
    "            signal = self.signals[:, i]\n",
    "            signal = signal.reshape(1, -1)\n",
    "\n",
    "            cut_signal = signal[:, int(len(signal[0]) - 256):len(signal[0])]\n",
    "            temp_data = np.insert(temp_data, temp_data.shape[1], cut_signal, axis=1)\n",
    "\n",
    "        self.signals = temp_data\n",
    "        ###\n",
    "\n",
    "    ### augmentation methods ###\n",
    "    def jittering_augmentation(self, signal):\n",
    "        # todo: add noise only after time of core bounce\n",
    "        # noise_start_time = 203\n",
    "        noise = np.random.normal(0, 1, signal.shape[1])\n",
    "        jittered_signal = signal + noise\n",
    "    \n",
    "        return jittered_signal\n",
    "    \n",
    "    def shift_augmentation(self, signal):\n",
    "        shift = np.random.normal(0, 50, 1)\n",
    "        shifted_signal = np.roll(signal, int(shift))\n",
    "        \n",
    "        return shifted_signal\n",
    "\n",
    "    def scale_augmentation(self, signal):\n",
    "        scale_factor = np.random.normal(1, 0.5, 1)\n",
    "        scale_factor = np.maximum(scale_factor, 0)\n",
    "        scaled_signal = scale_factor * signal\n",
    "        return scaled_signal\n",
    "\n",
    "    def mixture_augmentation(self, signal_1, signal_2):\n",
    "        distance_multiplier = np.random.normal(0.5, 0.2, 1)\n",
    "        # clip signal to range [0,1] as this is the multiplier by the normalised difference in signals\n",
    "        distance_multiplier = np.clip(distance_multiplier, 0, 1)\n",
    "        mixture_signal = signal_1 + distance_multiplier * (signal_2 - signal_1)\n",
    "\n",
    "        return mixture_signal\n",
    "\n",
    "    def window_warping_augmentation(self, signal):\n",
    "        # take window size of 10% of the signal with a warping factor of 2 or 0.5 (from literature)\n",
    "        warping_factor =  np.random.choice([0.5, 2])\n",
    "        # warping_factor = 0.5\n",
    "\n",
    "        window_size = math.floor(signal.shape[1] / 10)\n",
    "        scaled_window_size = warping_factor * window_size\n",
    "\n",
    "        # don't warp anything a little bit before the core-bounce - preserves core-bounce position\n",
    "        window_min_idx = 53\n",
    "\n",
    "        # find random reference position for start of window warping\n",
    "        window_start_idx = np.random.randint(window_min_idx, signal.shape[1] - scaled_window_size*2)\n",
    "        window_end_idx = window_start_idx + window_size\n",
    "\n",
    "        # select between warping by factor 1/2 or 2\n",
    "        if (warping_factor == 2):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:int(signal.shape[1]-(window_size))]\n",
    "\n",
    "            # time points\n",
    "            t = np.arange(len(signal_window))\n",
    "            warped_t = np.arange(0, len(signal_window), 0.5)\n",
    "\n",
    "            # interpolation for window warping\n",
    "            signal_window_warped = np.interp(warped_t, t, signal_window)\n",
    "\n",
    "            # combine signals\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        elif (warping_factor == 0.5):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:]\n",
    "            # add values to end of signal to make up for downsampled window\n",
    "            signal_after_window = np.pad(signal_after_window, (0, int(window_size - scaled_window_size)), mode='edge')\n",
    "\n",
    "            signal_window_warped = signal_window[::int(1/warping_factor)]\n",
    "\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        else:\n",
    "            warped_signal = signal\n",
    "\n",
    "        return warped_signal\n",
    "\n",
    "    ### critical functions ###\n",
    "    def calc_stats(self):\n",
    "        self.mean = self.signals.mean()\n",
    "        print('Signal Dataset mean: ',  self.mean)\n",
    "        self.std = np.std(self.signals, axis=None)\n",
    "        print('Signal Dataset std: ',  self.std)\n",
    "        self.scaling_factor = 5\n",
    "        print('Signal Dataset scaling factor (to match noise in generator): ',  self.scaling_factor)\n",
    "\n",
    "    def get_common(self):\n",
    "        self.common_ylim_signal = (self.signals[:,:].min(), self.signals[:,:].max())\n",
    "        return self.common_ylim_signal\n",
    "    \n",
    "    def standardize(self, signal):\n",
    "        standardized_signal = (signal - self.mean) / self.std\n",
    "        standardized_signal = standardized_signal / self.scaling_factor\n",
    "        return standardized_signal\n",
    "\n",
    "    def augmentation(self, desired_augmented_data_count):\n",
    "        while self.signals.shape[1] < desired_augmented_data_count:\n",
    "            idx_1 = np.random.randint(0, self.signals.shape[1])\n",
    "            signal_1 = self.signals[:, idx_1]\n",
    "            signal_1 = signal_1.reshape(1, -1)\n",
    "\n",
    "            ### mixture augmentation only ###\n",
    "            # find the class of signal_1 (assuming class is a column in self.parameters)\n",
    "            beta_class_of_signal_1 = np.argmax(self.parameters[idx_1, :])\n",
    "            # sample only from the same class for signal_2 and make sure it's not the same as signal_1\n",
    "            candidate_indices = [x for x in range(0, 1764) if x != idx_1 and np.argmax(self.parameters[x, :]) == beta_class_of_signal_1]\n",
    "            idx_2 = np.random.choice(candidate_indices)\n",
    "            signal_2 = self.signals[:, idx_2]\n",
    "            signal_2 = signal_2.reshape(1, -1)\n",
    "\n",
    "            # call selected augmentation function here\n",
    "            # augmented_signal = self.window_warping_augmentation(signal_1)\n",
    "            augmented_signal = self.mixture_augmentation(signal_1, signal_2)\n",
    "\n",
    "            self.augmented_data = np.insert(self.augmented_data, self.augmented_data.shape[1], augmented_signal, axis=1)\n",
    "            self.signals = np.insert(self.signals, self.signals.shape[1], augmented_signal, axis=1)\n",
    "\n",
    "            # just copy parameters for now\n",
    "            augmented_parameter = self.parameters[idx_1, :]\n",
    "\n",
    "            self.augmented_parameters = np.insert(self.augmented_parameters, self.augmented_parameters.shape[0], augmented_parameter, axis=0)\n",
    "            self.parameters = np.insert(self.parameters, self.parameters.shape[0], augmented_parameter, axis=0)\n",
    "\n",
    "        print(\"Signal Dataset Size after Data Augmentation: \", self.signals.shape)\n",
    "        print(\"Parameter Dataset Size after Data Augmentation: \", self.parameters.shape)\n",
    "\n",
    "    ### overloads ###\n",
    "    def __len__(self):\n",
    "        return self.signals.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[:, idx]\n",
    "        signal = signal.reshape(1, -1)\n",
    "\n",
    "        signal_standardized = self.standardize(signal)\n",
    "\n",
    "        return signal_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveforms_standardised(real_signals_batch):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each signal on a separate subplot\n",
    "    for i, ax in enumerate(axes):\n",
    "        x = np.arange(real_signals_batch.size(dim=2))\n",
    "        y = real_signals_batch[i, :, :].flatten()\n",
    "        ax.plot(x, y, color='blue')\n",
    "\n",
    "        ax.axvline(x=53, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.grid(True)\n",
    "        ax.set_ylim((-4, 2))\n",
    "        \n",
    "        # Add axis titles\n",
    "        ax.set_ylabel('distance * strain (cm^2)')\n",
    "        ax.set_xlabel('n (timestamps)')\n",
    "\n",
    "        parameters = real_parameter_batch[i, :].numpy()[0]\n",
    "\n",
    "        # parameters_with_names = f'{parameter_names[0]}: {parameters[0]:.6f}\\n{parameter_names[1]}: {parameters[1]:.2f}, {parameter_names[2]}: {parameters[2]:.2f}'\n",
    "        # ax.set_xlabel(f'Parameters:\\n{parameters_with_names}')\n",
    "\n",
    "    for i in range(407, 8 * 4):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('../plots/signal-manipulation/gw_signals_standardised_cut.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_waveforms(real_signals_batch):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each signal on a separate subplot\n",
    "    for i, ax in enumerate(axes):\n",
    "        x = np.arange(real_signals_batch.size(dim=2))\n",
    "        y = real_signals_batch[i, :, :].flatten()\n",
    "        y = y * dataset.scaling_factor\n",
    "        y = y * dataset.std + dataset.mean\n",
    "        ax.plot(x, y, color='blue')\n",
    "\n",
    "        ax.axvline(x=53, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.grid(True)\n",
    "        ax.set_ylim(ylim_signal)\n",
    "        \n",
    "        # Add axis titles\n",
    "        ax.set_ylabel('distance * strain (cm^2)')\n",
    "        ax.set_xlabel('n (timestamps)')\n",
    "\n",
    "        # Get parameter values as a NumPy array\n",
    "        parameters = real_parameter_batch[i, :].numpy()[0]\n",
    "\n",
    "        # Combine parameter names and values, format as a string\n",
    "        # parameters_with_names = f'{parameter_names[0]}: {parameters[0]:.6f}\\n{parameter_names[1]}: {parameters[1]:.2f}, {parameter_names[2]}: {parameters[2]:.2f}'\n",
    "        # ax.set_xlabel(f'Parameters:\\n{parameters_with_names}')\n",
    "\n",
    "    for i in range(407, 8 * 4):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('../plots/signal-manipulation/gw_signals_cut.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GWDataset(\"../data/input/richers_1764.csv\", \"../data/input/richers_1764_parameters.csv\")\n",
    "\n",
    "ylim_signal = dataset.get_common()\n",
    "# dataset.augmentation(6000)\n",
    "dataset.calc_stats()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "real_signals_batch  = next(iter(dataloader))\n",
    "\n",
    "print(\"Signal Batch Size: \", real_signals_batch.shape)\n",
    "\n",
    "ylim_signal_standardised = dataset.get_common()\n",
    "\n",
    "plot_waveforms_standardised(real_signals_batch)\n",
    "# plot_waveforms_standardised(real_signals_batch, real_parameter_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation (Random Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gans_type == 'dcgans':\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.ConvTranspose1d(nz, ngf * 32, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm1d(ngf * 32),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf * 32, ngf * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ngf * 16),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf * 16, ngf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ngf * 8),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ngf * 4),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ngf * 2),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ngf),\n",
    "                nn.LeakyReLU(True),\n",
    "\n",
    "                nn.ConvTranspose1d(ngf, nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            z = self.main(z)\n",
    "            return z\n",
    "        \n",
    "if gans_type == 'gans':\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(nz, ngf),\n",
    "                nn.BatchNorm1d(ngf),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Linear(ngf, ngf * 2),\n",
    "                nn.BatchNorm1d(ngf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Linear(ngf * 2, ngf * 4),\n",
    "                nn.BatchNorm1d(ngf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Linear(ngf * 4, ngf * 8),\n",
    "                nn.BatchNorm1d(ngf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Linear(ngf * 8, 512),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            z = z.view(z.size(0), -1)  # Flatten the tensor\n",
    "            z = self.main(z)\n",
    "            z = z.view(z.size(0), 1, 512)\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)\n",
    "\n",
    "model = Generator()\n",
    "summary(model, input_size=(nz, nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (gans_type == \"dcgans\"):\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Conv1d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout1d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 2),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout1d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 2, ndf * 4, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 4),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout1d(0.2),\n",
    "\n",
    "                nn.Conv1d(ndf * 4, ndf * 8, kernel_size=4,\n",
    "                        stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout1d(0.2),\n",
    "\n",
    "                ### Can increase model complexity here ###\n",
    "                # nn.Conv1d(ndf * 8, ndf * 16, kernel_size=4,\n",
    "                #         stride=2, padding=1, bias=False),\n",
    "                # nn.BatchNorm1d(ndf * 16),\n",
    "                # nn.LeakyReLU(0.2, inplace=True),\n",
    "                # nn.Dropout1d(0.2),\n",
    "\n",
    "                # nn.Conv1d(ndf * 16, ndf * 32, kernel_size=4,\n",
    "                #         stride=2, padding=1, bias=False),\n",
    "                # nn.BatchNorm1d(ndf * 32),\n",
    "                # nn.LeakyReLU(0.2, inplace=True),\n",
    "                # nn.Dropout1d(0.2),\n",
    "\n",
    "                # nn.Conv1d(ndf * 32, nc, kernel_size=4,\n",
    "                #         stride=2, padding=0, bias=False),\n",
    "                # nn.BatchNorm1d(ndf * 64),\n",
    "                # nn.LeakyReLU(0.2, inplace=True),\n",
    "                # nn.Dropout1d(0.2),\n",
    "                \n",
    "                # nn.Conv1d(ndf * 64, nc, kernel_size=4,\n",
    "                #         stride=2, padding=0, bias=False)\n",
    "            )\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.LazyLinear(1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.main(x)\n",
    "            x = x.view(x.shape[0], -1)  # Flatten the tensor\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "elif (gans_type == \"gans\"):\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "\n",
    "                nn.Linear(256, 128),\n",
    "                # nn.BatchNorm1d(128),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "\n",
    "                nn.Linear(128, 64),\n",
    "                # nn.BatchNorm1d(64),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "\n",
    "                nn.Linear(64, 32),\n",
    "                # nn.BatchNorm1d(32),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "\n",
    "                nn.Linear(32, 16),\n",
    "                # nn.BatchNorm1d(16),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "\n",
    "                nn.Linear(16, 1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "            x = self.main(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator().to(device)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)\n",
    "\n",
    "model = Discriminator()\n",
    "summary(model, input_size=(nc, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr_d, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
    "\n",
    "# learning-rate decay scheduler\n",
    "schedulerD = lr_scheduler.LinearLR(optimizerD, start_factor=1.0, end_factor=0.5, total_iters=32)\n",
    "schedulerG = lr_scheduler.LinearLR(optimizerG, start_factor=1.0, end_factor=0.5, total_iters=32)\n",
    "\n",
    "# establish convention for real and fake labels during training\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, device=device)\n",
    "\n",
    "num_cols = 4\n",
    "num_rows = 4\n",
    "\n",
    "# plot generated signals before training\n",
    "with torch.no_grad():\n",
    "    fake_signals = netG(fixed_noise).detach().cpu()\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # plot each signal on a separate subplot\n",
    "    for i, ax in enumerate(axes):\n",
    "        x = [i / 4096 for i in range(0, 256)]\n",
    "        x = [value - (53/4096) for value in x]\n",
    "        y = fake_signals[i, :, :].flatten()\n",
    "        y = y * dataset.scaling_factor\n",
    "        y = y * dataset.std + dataset.mean\n",
    "        ax.set_ylim(-600, 300)\n",
    "        ax.plot(x, y, color='red')\n",
    "\n",
    "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Remove y-axis ticks for the right-hand column\n",
    "        if i % num_cols == num_cols - 1:\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "        \n",
    "        # Remove x-axis tick labels for all but the bottom two plots\n",
    "        if i <= 11:\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "\n",
    "    for i in range(512, 8*4):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.supxlabel('time (s)', fontsize=24)\n",
    "    fig.supylabel('distance x strain (cm)', fontsize=24)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "signal_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_gradients = []\n",
    "G_gradients = []\n",
    "iters = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if (gans_type == 'dcgans' or gans_type == 'gans'):\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            # Format batch\n",
    "            real_gpu = data.to(device)\n",
    "            b_size = real_gpu.size(0)\n",
    "            label_real = torch.FloatTensor(b_size).uniform_(1.0, 1.0).to(device)\n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_gpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label_real)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, 1, device=device)\n",
    "            # Generate fake signal batch with G\n",
    "            fake = netG(noise)\n",
    "            label_fake = torch.FloatTensor(b_size).uniform_(0.0, 0.25).to(device)\n",
    "            # label_fake = torch.FloatTensor(b_size).uniform_(0.0, 0.0).to(device)\n",
    "            # Classify all fake batch with D\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label_fake)\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "            # Calculate gradients of discriminator parameters\n",
    "            D_gradients.append([param.grad.norm().item() for param in netD.parameters()])\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label_real = torch.FloatTensor(b_size).uniform_(1.0, 1.0).to(device)\n",
    "            # label_real = 1.0 - label_fake\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label_real)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "            # Calculate gradients of generator parameters\n",
    "            G_gradients.append([param.grad.norm().item() for param in netG.parameters()])\n",
    "\n",
    "            # Output training stats\n",
    "            if i % 50 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                    % (epoch, num_epochs, i, len(dataloader),\n",
    "                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "            iters += 1 \n",
    "        \n",
    "        # learning-rate decay\n",
    "        before_lr = optimizerD.param_groups[0][\"lr\"]\n",
    "        schedulerD.step()\n",
    "        after_lr = optimizerD.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD Discriminator lr %.7f -> %.7f\" % (epoch, before_lr, after_lr))\n",
    "\n",
    "        before_lr = optimizerG.param_groups[0][\"lr\"]\n",
    "        schedulerG.step()\n",
    "        after_lr = optimizerG.param_groups[0][\"lr\"]\n",
    "        print(\"Epoch %d: SGD Generator lr %.7f -> %.7f\" % (epoch, before_lr, after_lr))\n",
    "\n",
    "        # plot generated signals before training\n",
    "        with torch.no_grad():\n",
    "            fake_signals = netG(fixed_noise).detach().cpu()\n",
    "            fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            # plot each signal on a separate subplot\n",
    "            for i, ax in enumerate(axes):\n",
    "                x = [i / 4096 for i in range(0, 256)]\n",
    "                x = [value - (53/4096) for value in x]\n",
    "                y = fake_signals[i, :, :].flatten()\n",
    "                y = y * dataset.scaling_factor\n",
    "                y = y * dataset.std + dataset.mean\n",
    "                ax.set_ylim(-600, 300)\n",
    "                ax.plot(x, y, color='red')\n",
    "\n",
    "                ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "                ax.grid(True)\n",
    "                \n",
    "                # Remove y-axis ticks for the right-hand column\n",
    "                if i % num_cols == num_cols - 1:\n",
    "                    ax.yaxis.set_ticklabels([])\n",
    "                \n",
    "                # Remove x-axis tick labels for all but the bottom two plots\n",
    "                if i <= 11:\n",
    "                    ax.xaxis.set_ticklabels([])\n",
    "\n",
    "            for i in range(512, 8*4):\n",
    "                fig.delaxes(axes[i])\n",
    "\n",
    "            fig.supxlabel('time (s)', fontsize=24)\n",
    "            fig.supylabel('distance x strain (cm)', fontsize=24)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        signal_list.append(fake_signals)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training Time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of layers in the discriminator\n",
    "D_gradients = np.array(D_gradients)\n",
    "num_layers = D_gradients.shape[1]\n",
    "\n",
    "# Plot the gradients over training epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_layers):\n",
    "    # Calculate alpha value based on layer index\n",
    "    alpha = 1 - (i / num_layers)  # Higher layers are more transparent\n",
    "    plt.plot(D_gradients[:, i], label=f'Layer {i}', alpha=alpha, color=(1, 0, 0, alpha))\n",
    "\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Discriminator Gradients')\n",
    "plt.legend()\n",
    "# plt.savefig(\"../plots/architecture/gw_d_gradients_status_quo_32e.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of layers in the discriminator\n",
    "G_gradients = np.array(G_gradients)\n",
    "num_layers = G_gradients.shape[1]\n",
    "\n",
    "# Plot the gradients over training epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_layers):\n",
    "    # Calculate alpha value based on layer index\n",
    "    alpha = 1 - (i / num_layers)  # Higher layers are more transparent\n",
    "    plt.plot(G_gradients[:, i], label=f'Layer {i}', alpha=alpha, color=(0, 0, 1, alpha))\n",
    "\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Generator Gradients')\n",
    "plt.legend()\n",
    "# plt.savefig(\"../plots/architecture/gw_g_gradients_status_quo_32e.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(G_losses, label=\"Generator Loss\")\n",
    "plt.plot(D_losses, label=\"Discriminator Loss\")\n",
    "plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Discriminator Loss Convergence Point')\n",
    "plt.xlabel(\"Batch\", size=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.ylim(0, 5)\n",
    "plt.legend(fontsize=16)\n",
    "# plt.savefig(\"../plots/architecture/dcgans_training_loss_128e.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG, \"../models/stellar_core_collapse_signal_generator_dcgans.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
